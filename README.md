# LLM101_pathway
This uses the structure of Karpathy's LLM101n course (upcoming) and grows from top to bottom concepts with sample codes and explanations (my persepective).
If your intrested in learning with me, kindly request for contrib and lets learn LLM together from ground up. I personally feel only strong foundations provide great and feasible novelties.


Syllabus (By Karpathy, Not me Lol)

    Chapter 01 Bigram Language Model (language modeling)
    Chapter 02 Micrograd (machine learning, backpropagation)
    Chapter 03 N-gram model (multi-layer perceptron, matmul, gelu)
    Chapter 04 Attention (attention, softmax, positional encoder)
    Chapter 05 Transformer (transformer, residual, layernorm, GPT-2)
    Chapter 06 Tokenization (minBPE, byte pair encoding)
    Chapter 07 Optimization (initialization, optimization, AdamW)
    Chapter 08 Need for Speed I: Device (device, CPU, GPU, ...)
    Chapter 09 Need for Speed II: Precision (mixed precision training, fp16, bf16, fp8, ...)
    Chapter 10 Need for Speed III: Distributed (distributed optimization, DDP, ZeRO)
    Chapter 11 Datasets (datasets, data loading, synthetic data generation)
    Chapter 12 Inference I: kv-cache (kv-cache)
    Chapter 13 Inference II: Quantization (quantization)
    Chapter 14 Finetuning I: SFT (supervised finetuning SFT, PEFT, LoRA, chat)
    Chapter 15 Finetuning II: RL (reinforcement learning, RLHF, PPO, DPO)
    Chapter 16 Deployment (API, web app)
    Chapter 17 Multimodal (VQVAE, diffusion transformer)


For each and every chapters will we have set of theories, information and code files (Not at all limited to one solution or code!) and may contain sparse and scattered information. After all the chapters are done with seperate knowledge folders, we will have a combined project section fuses all the concepts, build and train an LLM from ground up.
